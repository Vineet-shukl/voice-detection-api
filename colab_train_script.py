# -*- coding: utf-8 -*-
"""Voice_Detection_Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xxxxxxxxx

# ðŸš€ Train AI Voice Detector on Google Colab

This notebook trains a voice detection model (Real vs AI) using free GPU resources.
Running this for 20-30 minutes will yield a fine-tuned model.

## 1. Install Dependencies
"""

!pip install -q transformers datasets evaluate accelerate librosa soundfile torch

"""## 2. Configuration"""

import os
import numpy as np
import torch
from datasets import load_dataset, Audio
from transformers import (
    AutoFeatureExtractor, 
    AutoModelForAudioClassification, 
    TrainingArguments, 
    Trainer
)
import evaluate

# We will use a public dataset from Hugging Face for "Real vs Fake" voice
# Dataset: "not-lain/deepfake-audio-dataset" (or similar)
DATASET_ID = "not-lain/deepfake-audio-dataset" 
MODEL_NAME = "facebook/wav2vec2-base" # Base model to fine-tune

# Settings
SAMPLE_RATE = 16000
BATCH_SIZE = 8
EPOCHS = 3 # 3 Epochs typically takes ~15-20 mins on Colab GPU

print(f"Using Device: {('cuda' if torch.cuda.is_available() else 'cpu')}")
if not torch.cuda.is_available():
    print("âš ï¸ WARNING: You are not using a GPU. Go to Runtime -> Change runtime type -> T4 GPU")

"""## 3. Load & Prepare Dataset"""

print("Downloading dataset...")
# Load dataset
dataset = load_dataset(DATASET_ID)

# Print info
print(dataset)

# Determine labels (Real vs Fake)
# Adjust map based on dataset structure
# For not-lain/deepfake-audio-dataset: Label 0 = Real, 1 = Fake (usually)
labels = dataset["train"].features["label"].names
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label

print(f"Labels: {label2id}")

# Preprocessing
feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME)

def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays, 
        sampling_rate=feature_extractor.sampling_rate, 
        max_length=SAMPLE_RATE * 4, # 4 seconds max
        truncation=True,
        padding=True # Ensure padding is applied
    )
    return inputs

# Resample audio to 16kHz
dataset = dataset.cast_column("audio", Audio(sampling_rate=SAMPLE_RATE))

# Tokenize
encoded_dataset = dataset.map(preprocess_function, batched=True, remove_columns=["audio", "file_path"] if "file_path" in dataset["train"].column_names else ["audio"])

# Split if no test set
if "test" not in encoded_dataset:
    encoded_dataset = encoded_dataset["train"].train_test_split(test_size=0.2)

"""## 4. Train Model"""

accuracy = evaluate.load("accuracy")

def compute_metrics(eval_pred):
    predictions = np.argmax(eval_pred.predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)

model = AutoModelForAudioClassification.from_pretrained(
    MODEL_NAME,
    num_labels=len(labels),
    label2id=label2id,
    id2label=id2label,
)

training_args = TrainingArguments(
    output_dir="voice_detection_model",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=3e-5,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"],
    eval_dataset=encoded_dataset["test"],
    tokenizer=feature_extractor,
    compute_metrics=compute_metrics,
)

print("Starting training (this may take 15-20 mins)...")
trainer.train()

"""## 5. Save & Download"""

output_path = "./fine_tuned_voice_model"
trainer.save_model(output_path)
feature_extractor.save_pretrained(output_path)

print("âœ… Model trained and saved!")

# Zip it for download
!zip -r voice_model.zip {output_path}

from google.colab import files
files.download("voice_model.zip")
